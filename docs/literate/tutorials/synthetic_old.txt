#
# Results Analysis
#
# Check the training differences for Q10 parameter
# This shows how close the model learned the true Q10 value
# out.train_diffs.Q10

# using Hyperopt
# using Distributed
# using WGLMakie

# mspempty = ModelSpec()

# nhyper = 4
# ho = @thyperopt for i in nhyper,
#         opt in [AdamW(0.01), AdamW(0.1), RMSProp(0.001), RMSProp(0.01)],
#         input_batchnorm in [true, false]

#     hyper_parameters = (; opt, input_batchnorm)
#     println("Hyperparameter run: \n", i, " of ", nhyper, "\t with hyperparameters \t", hyper_parameters, "\t")
#     out = EasyHybrid.tune(hybrid_model, df, mspempty; hyper_parameters..., nepochs = 10, plotting = false, show_progress = false, file_name = "test$i.jld2")
#     #out.best_loss
#     # out.best_loss, has to be first element of the tuple, return a rich record for this trial (stored in ho.results[i])
#     (out.best_loss, hyperps = hyper_parameters, ps_st = (out.ps, out.st), i = i)
# end

# losses = getfield.(ho.results, :best_loss)
# hyperps = getfield.(ho.results, :hyperps)

# # Helper function to make optimizer names short and readable
# function short_opt_name(opt)
#     if opt isa AdamW
#         return "AdamW(η=$(opt.eta))"
#     elseif opt isa RMSProp
#         return "RMSProp(η=$(opt.eta))"
#     else
#         return string(typeof(opt))
#     end
# end

# # Sort losses and associated data by increasing loss
# idx = sortperm(losses)
# sorted_losses = losses[idx]
# sorted_hyperps = hyperps[idx]

# fig = Figure(figure_padding = 50)
# # Prepare tick labels with hyperparameter info for each trial (sorted)
# sorted_ticklabels = [
#     join(
#             [
#                 k == :opt ? "opt=$(short_opt_name(v))" : "$k=$(repr(v))"
#                 for (k, v) in pairs(hp)
#             ], "\n"
#         )
#         for hp in sorted_hyperps
# ]
# ax = Makie.Axis(
#     fig[1, 1];
#     xlabel = "Trial",
#     ylabel = "Loss",
#     title = "Hyperparameter Tuning Results",
#     xgridvisible = false,
#     ygridvisible = false,
#     xticks = (1:length(sorted_losses), sorted_ticklabels),
#     xticklabelrotation = 45
# )
# scatter!(ax, 1:length(sorted_losses), sorted_losses; markersize = 15, color = :dodgerblue)

# # Get the best trial
# best_idx = argmin(losses)
# best_trial = ho.results[best_idx]

# best_params = best_trial.ps_st        # (ps, st)

# # Print the best hyperparameters
# printmin(ho)

# # Plot the results
# import Plots
# using Plots.PlotMeasures
# # rebuild the ho object as intended by plot function for hyperopt object
# ho2 = deepcopy(ho)
# ho2.results = getfield.(ho.results, :best_loss)

# Plots.plot(ho2, xrotation = 25, left_margin = [100mm 0mm], bottom_margin = 60mm, ylab = "loss", size = (900, 900))

# # Train the model with the best hyperparameters
# best_hyperp = best_hyperparams(ho)
# out = EasyHybrid.tune(hybrid_model, df, mspempty; best_hyperp..., nepochs = 100, train_from = best_params)